{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:09:38.749001Z","iopub.status.busy":"2023-08-31T08:09:38.748473Z","iopub.status.idle":"2023-08-31T08:09:39.859002Z","shell.execute_reply":"2023-08-31T08:09:39.857634Z","shell.execute_reply.started":"2023-08-31T08:09:38.748907Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: NVIDIA GeForce MX330 (UUID: GPU-997e1912-4b02-eb94-fde9-258b23b45a72)\n"]}],"source":["! nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:09:39.862280Z","iopub.status.busy":"2023-08-31T08:09:39.861943Z","iopub.status.idle":"2023-08-31T08:13:34.999968Z","shell.execute_reply":"2023-08-31T08:13:34.998837Z","shell.execute_reply.started":"2023-08-31T08:09:39.862250Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","# ! pip install -qq -U langchain tiktoken pypdf faiss-gpu\n","! pip install -qq -U langchain tiktoken faiss-gpu\n","! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n","! pip install -qq -U accelerate bitsandbytes xformers einops"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:13:35.001839Z","iopub.status.busy":"2023-08-31T08:13:35.001464Z","iopub.status.idle":"2023-08-31T08:13:53.945127Z","shell.execute_reply":"2023-08-31T08:13:53.943986Z","shell.execute_reply.started":"2023-08-31T08:13:35.001808Z"},"trusted":true},"outputs":[],"source":["pip install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:13:53.949118Z","iopub.status.busy":"2023-08-31T08:13:53.948404Z","iopub.status.idle":"2023-08-31T08:14:16.336201Z","shell.execute_reply":"2023-08-31T08:14:16.335103Z","shell.execute_reply.started":"2023-08-31T08:13:53.949076Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import glob\n","import textwrap\n","import time\n","\n","import langchain\n","\n","# loaders\n","# from langchain.document_loaders import PyPDFLoader\n","# from langchain.document_loaders import DirectoryLoader\n","from langchain.document_loaders import TextLoader\n","\n","\n","# splits\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# prompts\n","from langchain import PromptTemplate, LLMChain\n","\n","# vector stores\n","from langchain.vectorstores import FAISS\n","\n","# models\n","from langchain.llms import HuggingFacePipeline\n","from InstructorEmbedding import INSTRUCTOR\n","from langchain.embeddings import HuggingFaceInstructEmbeddings\n","\n","# retrievers\n","from langchain.chains import RetrievalQA\n","\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","print(langchain.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:14:16.338219Z","iopub.status.busy":"2023-08-31T08:14:16.337868Z","iopub.status.idle":"2023-08-31T08:14:16.569029Z","shell.execute_reply":"2023-08-31T08:14:16.567950Z","shell.execute_reply.started":"2023-08-31T08:14:16.338189Z"},"trusted":true},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import difflib\n","\n","# Function to search for a book by name and return the best match URL\n","def search_book_by_name(book_name):\n","    base_url = \"https://www.gutenberg.org/\"\n","    search_url = base_url + \"ebooks/search/?query=\" + book_name.replace(\" \", \"+\") + \"&submit_search=Go%21\"\n","    \n","    response = requests.get(search_url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","    # Find the best match link based on similarity ratio\n","    best_match_ratio = 0\n","    best_match_url = \"\"\n","\n","    for link in soup.find_all(\"li\", class_=\"booklink\"):\n","        link_title = link.find(\"span\", class_=\"title\").get_text()\n","        similarity_ratio = difflib.SequenceMatcher(None, book_name.lower(), link_title.lower()).ratio()\n","        if similarity_ratio > best_match_ratio:\n","            best_match_ratio = similarity_ratio\n","            best_match_url = base_url + link.find(\"a\").get(\"href\")\n","\n","    return best_match_url\n","\n","# Function to get the \"Plain Text UTF-8\" download link from the book page\n","def get_plain_text_link(book_url):\n","    response = requests.get(book_url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    \n","    plain_text_link = \"\"\n","    \n","    for row in soup.find_all(\"tr\"):\n","        format_cell = row.find(\"td\", class_=\"unpadded icon_save\")\n","        if format_cell and \"Plain Text UTF-8\" in format_cell.get_text():\n","            plain_text_link = format_cell.find(\"a\").get(\"href\")\n","            break\n","    \n","    return plain_text_link\n","\n","\n","# Function to get the content of the \"Plain Text UTF-8\" link\n","def get_plain_text_content(plain_text_link):\n","    response = requests.get(plain_text_link)\n","    content = response.text\n","    return content\n","\n","\n","# Main function\n","def load_book(book_name):\n","    best_match_url = search_book_by_name(book_name)\n","\n","    if best_match_url:\n","        plain_text_link = get_plain_text_link(best_match_url)\n","        if plain_text_link:\n","            full_plain_text_link = \"https://www.gutenberg.org\" + plain_text_link\n","            plain_text_content = get_plain_text_content(full_plain_text_link)\n","#             print(\"Plain Text UTF-8 content:\", plain_text_content)\n","            \n","            book_text = plain_text_content\n","            \n","            file = book_name + \".txt\"\n","\n","            # Remove the BOM character if it exists\n","            book_text = book_text.lstrip('\\ufeff')\n","\n","            # Choose an appropriate encoding, such as 'utf-8'\n","            with open(file, \"w\", encoding=\"utf-8\") as file:\n","                file.write(book_text)\n","                \n","            return book_text\n","        else:\n","            print(\"No Plain Text UTF-8 link found.\")\n","            return \"web site error\"\n","    else:\n","        print(\"No matching book found.\")\n","        return \"web site error\"\n","\n","# def clean_book_content(book_text):\n","#     cleaned_book_text = book_text.replace(\"\\n\", \" \")\n","#     cleaned_book_text = cleaned_book_text.replace(\"\\r\", \" \")\n","#     cleaned_book_text = cleaned_book_text.replace(\"\\ufeff\", \"\")\n","#     return cleaned_book_text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:14:16.571358Z","iopub.status.busy":"2023-08-31T08:14:16.570442Z","iopub.status.idle":"2023-08-31T08:14:16.579168Z","shell.execute_reply":"2023-08-31T08:14:16.577824Z","shell.execute_reply.started":"2023-08-31T08:14:16.571316Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # LLMs\n","    model_name = 'llama2-13b' # wizardlm, bloom, falcon, llama2-7b, llama2-13b\n","    temperature = 0,\n","    top_p = 0.95,\n","    repetition_penalty = 1.15    \n","\n","    # splitting\n","    split_chunk_size = 800\n","    split_overlap = 0\n","    \n","    # embeddings\n","    embeddings_model_repo = 'hkunlp/instructor-base'    \n","\n","    # similar passages\n","    k = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:14:16.581331Z","iopub.status.busy":"2023-08-31T08:14:16.580935Z","iopub.status.idle":"2023-08-31T08:14:16.596511Z","shell.execute_reply":"2023-08-31T08:14:16.595369Z","shell.execute_reply.started":"2023-08-31T08:14:16.581284Z"},"trusted":true},"outputs":[],"source":["def get_model(model = CFG.model_name):\n","\n","    print('\\nDownloading model: ', model, '\\n\\n')\n","\n","    if model == 'wizardlm':\n","        model_repo = 'TheBloke/wizardLM-7B-HF'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True\n","        )\n","        \n","        max_len = 1024\n","\n","    elif model == 'llama2-7b':\n","        model_repo = 'daryl149/llama-2-7b-chat-hf'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","        )\n","        \n","        max_len = 2048\n","\n","    elif model == 'llama2-13b':\n","        model_repo = 'daryl149/llama-2-13b-chat-hf'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","        )\n","        \n","        max_len = 8192\n","\n","    elif model == 'bloom':\n","        model_repo = 'bigscience/bloom-7b1'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","        )\n","        \n","        max_len = 1024\n","\n","    elif model == 'falcon':\n","        model_repo = 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","        )\n","        \n","        max_len = 1024\n","\n","    else:\n","        print(\"Not implemented model (tokenizer and backbone)\")\n","\n","    return tokenizer, model, max_len"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:33:29.027917Z","iopub.status.busy":"2023-08-31T08:33:29.026827Z","iopub.status.idle":"2023-08-31T08:36:45.301773Z","shell.execute_reply":"2023-08-31T08:36:45.300802Z","shell.execute_reply.started":"2023-08-31T08:33:29.027851Z"},"trusted":true},"outputs":[],"source":["%%time\n","\n","tokenizer, model, max_len = get_model(model = CFG.model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:37:00.038875Z","iopub.status.busy":"2023-08-31T08:37:00.038481Z","iopub.status.idle":"2023-08-31T08:37:00.061587Z","shell.execute_reply":"2023-08-31T08:37:00.060594Z","shell.execute_reply.started":"2023-08-31T08:37:00.038840Z"},"trusted":true},"outputs":[],"source":["pipe = pipeline(\n","    task = \"text-generation\",\n","    model = model,\n","    tokenizer = tokenizer,\n","    pad_token_id = tokenizer.eos_token_id,\n","    max_length = max_len,\n","    temperature = CFG.temperature,\n","    top_p = CFG.top_p,\n","    repetition_penalty = CFG.repetition_penalty\n",")\n","\n","llm = HuggingFacePipeline(pipeline = pipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:37:03.368207Z","iopub.status.busy":"2023-08-31T08:37:03.367804Z","iopub.status.idle":"2023-08-31T08:37:03.377407Z","shell.execute_reply":"2023-08-31T08:37:03.376267Z","shell.execute_reply.started":"2023-08-31T08:37:03.368177Z"},"trusted":true},"outputs":[],"source":["llm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:37:05.898967Z","iopub.status.busy":"2023-08-31T08:37:05.898575Z","iopub.status.idle":"2023-08-31T08:38:03.634627Z","shell.execute_reply":"2023-08-31T08:38:03.633682Z","shell.execute_reply.started":"2023-08-31T08:37:05.898930Z"},"trusted":true},"outputs":[],"source":["### testing model, not using the book yet\n","### answer is not necessarily related to the book\n","query = \"Give me 5 examples of cool potions and explain what they do\"\n","llm(query)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:03.637135Z","iopub.status.busy":"2023-08-31T08:38:03.636601Z","iopub.status.idle":"2023-08-31T08:38:03.642405Z","shell.execute_reply":"2023-08-31T08:38:03.641431Z","shell.execute_reply.started":"2023-08-31T08:38:03.637099Z"},"trusted":true},"outputs":[],"source":["prompt_template = \"\"\"\n","Don't try to make up an answer, if you don't know just say that you don't know.\n","Answer in the same language the question was asked.\n","Use only the following pieces of context to answer the question at the end.\n","\n","{context}\n","\n","Question: {question}\n","Answer:\"\"\"\n","\n","\n","PROMPT = PromptTemplate(\n","    template = prompt_template, \n","    input_variables = [\"context\", \"question\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:03.644622Z","iopub.status.busy":"2023-08-31T08:38:03.644046Z","iopub.status.idle":"2023-08-31T08:38:03.655346Z","shell.execute_reply":"2023-08-31T08:38:03.654311Z","shell.execute_reply.started":"2023-08-31T08:38:03.644586Z"},"trusted":true},"outputs":[],"source":["def loadForEmbeddings(book_txt):\n","    # load document\n","    loader = TextLoader(book_txt, encoding=\"utf-8\")\n","    documents = loader.load()\n","    \n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size = CFG.split_chunk_size,\n","        chunk_overlap = CFG.split_overlap\n","    )\n","\n","    texts = text_splitter.split_documents(documents)\n","    return texts\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:03.658617Z","iopub.status.busy":"2023-08-31T08:38:03.658239Z","iopub.status.idle":"2023-08-31T08:38:03.668594Z","shell.execute_reply":"2023-08-31T08:38:03.667586Z","shell.execute_reply.started":"2023-08-31T08:38:03.658585Z"},"trusted":true},"outputs":[],"source":["def wrap_text_preserve_newlines(text, width=200): # 110\n","    # Split the input text into lines based on newline characters\n","    lines = text.split('\\n')\n","\n","    # Wrap each line individually\n","    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n","\n","    # Join the wrapped lines back together using newline characters\n","    wrapped_text = '\\n'.join(wrapped_lines)\n","\n","    return wrapped_text\n","\n","\n","def process_llm_response(llm_response):\n","    ans = wrap_text_preserve_newlines(llm_response['result'])\n","    \n","    sources_used = llm_response['source_documents'][0].metadata['source']\n","    \n","    ans = ans + '\\n\\nSources: \\n' + sources_used\n","    return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:03.670294Z","iopub.status.busy":"2023-08-31T08:38:03.669939Z","iopub.status.idle":"2023-08-31T08:38:03.687128Z","shell.execute_reply":"2023-08-31T08:38:03.686122Z","shell.execute_reply.started":"2023-08-31T08:38:03.670259Z"},"trusted":true},"outputs":[],"source":["def llm_ans(query):\n","    start = time.time()\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm = llm,\n","        chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n","        retriever = retriever, \n","        chain_type_kwargs = {\"prompt\": PROMPT},\n","        return_source_documents = True,\n","        verbose = False\n","    )\n","    llm_response = qa_chain(query)\n","    ans = process_llm_response(llm_response)\n","    end = time.time()\n","\n","    time_elapsed = int(round(end - start, 0))\n","    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n","    return ans + time_elapsed_str"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:03.688991Z","iopub.status.busy":"2023-08-31T08:38:03.688615Z","iopub.status.idle":"2023-08-31T08:38:16.210183Z","shell.execute_reply":"2023-08-31T08:38:16.209159Z","shell.execute_reply.started":"2023-08-31T08:38:03.688958Z"},"trusted":true},"outputs":[],"source":["import gradio as gr\n","\n","title = \"GutenbergChat Hub\"\n","vectordb = None\n","retriever = None\n","\n","# Submit book\n","def submit_book(book_name):\n","    global vectordb, retriever\n","    if not book_name:\n","        return \"Please enter the name of the book.\"\n","    \n","    book_text = load_book(book_name)\n","    file = book_name + \".txt\"\n","    texts = loadForEmbeddings(file)\n","\n","    ### download embeddings model\n","    instructor_embeddings = HuggingFaceInstructEmbeddings(\n","        model_name = CFG.embeddings_model_repo,\n","        model_kwargs = {\"device\": \"cuda\"}\n","    )\n","\n","    ### create embeddings and DB\n","    vectordb = FAISS.from_documents(\n","        documents = texts, \n","        embedding = instructor_embeddings\n","    )\n","\n","    ### persist vector database\n","    vectordb.save_local(\"faiss_index_hp\")\n","    \n","    retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n","\n","    \n","    return \"done\"\n","\n","\n","# Q/A\n","def get_response(prompt):\n","#     if (not book_name and not prompt):\n","#         return \"Please enter the name of the book and the prompt.\"\n","#     if not book_name:\n","#         return \"Please enter the name of the book.\"\n","#     if not prompt:\n","#         return \"Please enter the prompt.\"\n","    query = prompt\n","#     llm_response = qa_chain(query)\n","    \n","    return llm_ans(query)\n","\n","# Interface 1\n","submitBook = gr.Interface(fn=submit_book, inputs=\"text\", outputs=\"text\", title=\"Submit your book here first\")\n","# Interface 2\n","chatBot = gr.Interface(\n","        fn=get_response,\n","        inputs=\"text\",\n","        outputs=\"text\",\n","        title=title + \" - Q/A-Bot\",\n","        description=\"Enter the name of the book in previous tab, then ask questions here\",\n","        examples=[\"What are the characters in the book?\"]\n","    )\n","\n","demo = gr.TabbedInterface([submitBook, chatBot], [\"SubmitBook\", \"Q/A-Bot\"])\n","demo.launch()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ### test if vector DB was loaded correctly\n","# vectordb.similarity_search('magic creatures')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ### testing MMR search\n","# question = \"What are the characters\"\n","# vectordb.max_marginal_relevance_search(question, k = CFG.k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Gradio app\n","# def chatbot_interface(book_name, query):\n","#     if not book_name:\n","#         return \"Please enter the name of the book.\"\n","    \n","#     book_text = load_book(book_name)  # Implement the function to load the book\n","    \n","#     # Generate book summary\n","#     book_summary = generate_summary(book_text)\n","    \n","#     llm_response = qa_chain(query)  # Implement the function to get the LLM response\n","    \n","#     return f\"Book Summary: {book_summary}\\n\\nYou: {query}\\nBot: {llm_response}\"\n","\n","# if __name__ == \"__main__\":\n","#     iface = gr.Interface(\n","#         fn=chatbot_interface,\n","#         inputs=[\"text\", \"text\"],\n","#         outputs=\"text\",\n","#         title=\"Chatbot Gradio Web App\",\n","#         description=\"Enter the name of the book, then ask questions to the chatbot.\",\n","#         examples=[[\"Book Name\", \"What are the characters in the book?\"]]\n","#     )\n","#     iface.launch()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pip install streamlit"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import streamlit as st\n","# from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# # Load pre-trained BART model and tokenizer\n","# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","\n","# # Function to generate book summary\n","# def generate_summary(book_text):\n","#     # Split the source document into chunks\n","#     chunk_size = 10000 # Adjust as needed\n","#     chunks = [book_text[i:i + chunk_size] for i in range(0, len(book_text), chunk_size)]\n","\n","#     # Generate summaries for each chunk\n","#     summaries = []\n","#     for chunk in chunks:\n","#         inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#         summary_ids = model.generate(inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#         summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","#         summaries.append(summary)\n","\n","#     # Combine the summaries of all chunks\n","#     combined_summary = \" \".join(summaries)\n","    \n","#     # Tokenize and summarize the combined summary\n","#     combined_inputs = tokenizer.encode(\"summarize: \" + combined_summary, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#     combined_summary_ids = model.generate(combined_inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#     final_summary = tokenizer.decode(combined_summary_ids[0], skip_special_tokens=True)\n","    \n","#     return final_summary\n","\n","# # Streamlit app\n","# def main():\n","#     st.title(\"Chatbot Web App\")\n","    \n","#     # Input for the book name\n","#     book_name = st.text_input(\"Enter the name of the book:\")\n","    \n","#     if book_name:\n","#         book_text = load_book(book_name)  # Implement the function to load the book\n","        \n","#         # Generate and display the book summary\n","#         book_summary = generate_summary(book_text)\n","#         st.subheader(\"Book Summary:\")\n","#         st.write(book_summary)\n","#     else:\n","#         st.warning(\"Please enter the name of the book.\")\n","    \n","#     st.sidebar.title(\"Chatbot\")\n","    \n","#     # Interactive chatbot section\n","#     query = st.sidebar.text_input(\"Enter your question:\")\n","    \n","#     if query:\n","#         llm_response = qa_chain(query)  # Implement the function to get the LLM response\n","#         st.sidebar.subheader(\"You:\")\n","#         st.sidebar.write(query)\n","        \n","#         st.sidebar.subheader(\"Bot:\")\n","#         st.sidebar.write(llm_response)\n","\n","# if __name__ == \"__main__\":\n","#     main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !streamlit run /opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pip install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import gradio as gr\n","# from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# # Load pre-trained BART model and tokenizer\n","# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","\n","# # Function to generate book summary\n","# def generate_summary(book_text):\n","#     # Split the source document into chunks\n","#     chunk_size = 10000 # Adjust as needed\n","#     chunks = [book_text[i:i + chunk_size] for i in range(0, len(book_text), chunk_size)]\n","\n","#     # Generate summaries for each chunk\n","#     summaries = []\n","#     for chunk in chunks:\n","#         inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#         summary_ids = model.generate(inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#         summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","#         summaries.append(summary)\n","\n","#     # Combine the summaries of all chunks\n","#     combined_summary = \" \".join(summaries)\n","    \n","#     # Tokenize and summarize the combined summary\n","#     combined_inputs = tokenizer.encode(\"summarize: \" + combined_summary, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#     combined_summary_ids = model.generate(combined_inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#     final_summary = tokenizer.decode(combined_summary_ids[0], skip_special_tokens=True)\n","    \n","#     return final_summary\n","\n","# # Gradio app\n","# def chatbot_interface(book_name, query):\n","#     if not book_name:\n","#         return \"Please enter the name of the book.\"\n","    \n","#     book_text = load_book(book_name)  # Implement the function to load the book\n","    \n","#     # Generate book summary\n","#     book_summary = generate_summary(book_text)\n","    \n","#     llm_response = qa_chain(query)  # Implement the function to get the LLM response\n","    \n","#     return f\"Book Summary: {book_summary}\\n\\nYou: {query}\\nBot: {llm_response}\"\n","\n","# if __name__ == \"__main__\":\n","#     iface = gr.Interface(\n","#         fn=chatbot_interface,\n","#         inputs=[\"text\", \"text\"],\n","#         outputs=\"text\",\n","#         title=\"Chatbot Gradio Web App\",\n","#         description=\"Enter the name of the book, then ask questions to the chatbot.\",\n","#         examples=[[\"Book Name\", \"What are the characters in the book?\"]]\n","#     )\n","#     iface.launch()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import requests\n","# from bs4 import BeautifulSoup\n","# import difflib\n","\n","# # Function to search for a book by name and return the best match URL\n","# def search_book_by_name(book_name):\n","#     base_url = \"https://www.gutenberg.org/\"\n","#     search_url = base_url + \"ebooks/search/?query=\" + book_name.replace(\" \", \"+\") + \"&submit_search=Go%21\"\n","    \n","#     response = requests.get(search_url)\n","#     soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","#     # Find the best match link based on similarity ratio\n","#     best_match_ratio = 0\n","#     best_match_url = \"\"\n","\n","#     for link in soup.find_all(\"li\", class_=\"booklink\"):\n","#         link_title = link.find(\"span\", class_=\"title\").get_text()\n","#         similarity_ratio = difflib.SequenceMatcher(None, book_name.lower(), link_title.lower()).ratio()\n","#         if similarity_ratio > best_match_ratio:\n","#             best_match_ratio = similarity_ratio\n","#             best_match_url = base_url + link.find(\"a\").get(\"href\")\n","\n","#     return best_match_url\n","\n","# # Function to get the \"Plain Text UTF-8\" download link from the book page\n","# def get_plain_text_link(book_url):\n","#     response = requests.get(book_url)\n","#     soup = BeautifulSoup(response.content, \"html.parser\")\n","    \n","#     plain_text_link = \"\"\n","    \n","#     for row in soup.find_all(\"tr\"):\n","#         format_cell = row.find(\"td\", class_=\"unpadded icon_save\")\n","#         if format_cell and \"Plain Text UTF-8\" in format_cell.get_text():\n","#             plain_text_link = format_cell.find(\"a\").get(\"href\")\n","#             break\n","    \n","#     return plain_text_link\n","\n","\n","# # Function to get the content of the \"Plain Text UTF-8\" link\n","# def get_plain_text_content(plain_text_link):\n","#     response = requests.get(plain_text_link)\n","#     content = response.text\n","#     return content\n","\n","\n","# # Main function\n","# def load_book(book_name):\n","#     best_match_url = search_book_by_name(book_name)\n","\n","#     if best_match_url:\n","#         plain_text_link = get_plain_text_link(best_match_url)\n","#         if plain_text_link:\n","#             full_plain_text_link = \"https://www.gutenberg.org\" + plain_text_link\n","#             plain_text_content = get_plain_text_content(full_plain_text_link)\n","# #             print(\"Plain Text UTF-8 content:\", plain_text_content)\n","            \n","#             book_text = plain_text_content\n","            \n","#             file = book_name + \".txt\"\n","\n","#             # Remove the BOM character if it exists\n","#             book_text = book_text.lstrip('\\ufeff')\n","\n","#             # Choose an appropriate encoding, such as 'utf-8'\n","#             with open(file, \"w\", encoding=\"utf-8\") as file:\n","#                 file.write(book_text)\n","                \n","#             return book_text\n","#         else:\n","#             print(\"No Plain Text UTF-8 link found.\")\n","#             return \"web site error\"\n","#     else:\n","#         print(\"No matching book found.\")\n","#         return \"web site error\"\n","\n","# # def clean_book_content(book_text):\n","# #     cleaned_book_text = book_text.replace(\"\\n\", \" \")\n","# #     cleaned_book_text = cleaned_book_text.replace(\"\\r\", \" \")\n","# #     cleaned_book_text = cleaned_book_text.replace(\"\\ufeff\", \"\")\n","# #     return cleaned_book_text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# # Load pre-trained BART model and tokenizer\n","# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","\n","# # Function to generate book summary\n","# def generate_summary(book_text):\n","#     # Split the source document into chunks\n","#     chunk_size = 100000 # Adjust as needed\n","#     chunks = [book_text[i:i + chunk_size] for i in range(0, len(book_text), chunk_size)]\n","\n","#     # Generate summaries for each chunk\n","#     summaries = []\n","#     for chunk in chunks:\n","#         inputs = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#         summary_ids = model.generate(inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#         summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","#         summaries.append(summary)\n","\n","#     # Combine the summaries of all chunks\n","#     combined_summary = \" \".join(summaries)\n","    \n","#     # Tokenize and summarize the combined summary\n","#     combined_inputs = tokenizer.encode(\"summarize: \" + combined_summary, return_tensors=\"pt\", max_length=1024, truncation=True)\n","#     combined_summary_ids = model.generate(combined_inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","#     final_summary = tokenizer.decode(combined_summary_ids[0], skip_special_tokens=True)\n","    \n","#     return final_summary"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:16.213788Z","iopub.status.busy":"2023-08-31T08:38:16.213509Z","iopub.status.idle":"2023-08-31T08:38:32.819401Z","shell.execute_reply":"2023-08-31T08:38:32.818483Z","shell.execute_reply.started":"2023-08-31T08:38:16.213762Z"},"trusted":true},"outputs":[],"source":["import re\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# Load pre-trained BART model and tokenizer\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","\n","# Function to generate book summary\n","def generate_summary(book_text):\n","\n","    # Define the possible variations of the start marker\n","    possible_start_markers = [\n","        r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK (.+?) \\*\\*\\*\",\n","        r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK (.+?) \\*\\*\\*\"\n","    ]\n","\n","    # Fetch the plain_text_content of the book (assuming you have it)\n","    plain_text_content = book_text  # Fetch the content here\n","\n","    start_index = None\n","    for start_marker_pattern in possible_start_markers:\n","        match = re.search(start_marker_pattern, plain_text_content)\n","        if match:\n","            start_index = match.start()\n","            book_name = match.group(1)\n","            break\n","\n","    if start_index is not None:\n","        end_marker = f\"*** END OF THE PROJECT GUTENBERG EBOOK {book_name} ***\"\n","\n","        end_index = plain_text_content.find(end_marker, start_index)\n","\n","        if end_index != -1:\n","            text_to_summarize = plain_text_content[start_index + len(match.group(0)):end_index]\n","\n","            # Encode the text using the tokenizer\n","            input_ids = tokenizer.encode(text_to_summarize, return_tensors='pt', max_length=1024, truncation=True)\n","\n","            # Generate the summary using the BART model, aiming for 200-300 words\n","            summary_ids = model.generate(input_ids, max_length=300, min_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n","\n","            # Decode the summary_ids to get the human-readable summary\n","            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","            # Print the generated summary\n","            print(\"Generated Summary:\", summary.strip())  # Use strip() to remove leading/trailing spaces\n","            return summary.strip()\n","        else:\n","            print(f\"End marker not found for {book_name}.\")\n","            return \"book content error\"\n","    else:\n","        print(\"Start marker not found.\")\n","        return \"book content error\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# load book\n","book_text = load_book(\"The changed brides\")\n","    \n","# Generate book summary\n","book_summary = generate_summary(book_text)\n","\n","print(book_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-31T08:38:32.821409Z","iopub.status.busy":"2023-08-31T08:38:32.821022Z","iopub.status.idle":"2023-08-31T08:38:47.559847Z","shell.execute_reply":"2023-08-31T08:38:47.558883Z","shell.execute_reply.started":"2023-08-31T08:38:32.821374Z"},"trusted":true},"outputs":[],"source":["import gradio as gr\n","\n","title = \"GutenbergChat Hub\"\n","\n","# Summary\n","def get_summary(book_name):\n","    if not book_name:\n","        return \"Please enter the name of the book.\"\n","    \n","    # load book\n","    book_text = load_book(book_name)\n","    \n","    # Generate book summary\n","    book_summary = generate_summary(book_text)\n","    \n","    return f\"Book Summary: {book_summary}\\n\"\n","\n","# TODO: Q/A\n","# Q/A\n","def get_response(book_name, prompt):\n","    if (not book_name and not prompt):\n","        return \"Please enter the name of the book and the prompt.\"\n","    if not book_name:\n","        return \"Please enter the name of the book.\"\n","    if not prompt:\n","        return \"Please enter the prompt.\"\n","    return \"#TODO \" + prompt\n","\n","# Interface 1\n","summaryBot = gr.Interface(fn=get_summary, inputs=\"text\", outputs=\"text\", title=title + \" - Summary\")\n","# Interface 2\n","chatbot = gr.Interface(fn=get_response, inputs=[\"text\", \"text\"], outputs=\"text\", title=title + \" - Q/A-Bot\")\n","\n","demo2 = gr.TabbedInterface([summaryBot, chatbot], [\"Summary\", \"Q/A-Bot\"])\n","demo2.launch()\n"]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
