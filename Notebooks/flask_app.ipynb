{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting replicate\n",
      "  Obtaining dependency information for replicate from https://files.pythonhosted.org/packages/67/6d/c0a5cad0a5907454580ad0b51d0bf82d0d0980590e569f1f0f37f28c2316/replicate-0.15.5-py3-none-any.whl.metadata\n",
      "  Downloading replicate-0.15.5-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from replicate) (23.1)\n",
      "Requirement already satisfied: pydantic>1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from replicate) (1.10.12)\n",
      "Collecting httpx<1,>=0.21.0 (from replicate)\n",
      "  Obtaining dependency information for httpx<1,>=0.21.0 from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (2023.7.22)\n",
      "Collecting httpcore<0.19.0,>=0.18.0 (from httpx<1,>=0.21.0->replicate)\n",
      "  Obtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from pydantic>1->replicate) (4.7.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.21.0->replicate) (3.5.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.21.0->replicate)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading replicate-0.15.5-py3-none-any.whl (25 kB)\n",
      "Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.7 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/75.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 75.7/75.7 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.0 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/76.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.0/76.0 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: h11, httpcore, httpx, replicate\n",
      "Successfully installed h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 replicate-0.15.5\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/10.8 MB 217.9 kB/s eta 0:00:50\n",
      "     --------------------------------------- 0.0/10.8 MB 245.8 kB/s eta 0:00:44\n",
      "     --------------------------------------- 0.1/10.8 MB 435.7 kB/s eta 0:00:25\n",
      "      -------------------------------------- 0.2/10.8 MB 612.6 kB/s eta 0:00:18\n",
      "      -------------------------------------- 0.2/10.8 MB 765.3 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.4/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.4/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.5/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.6/10.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/10.8 MB 1.3 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.7/10.8 MB 1.3 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 0.9/10.8 MB 1.5 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/10.8 MB 1.5 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/10.8 MB 1.5 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 1.0/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.1/10.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.2/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.3/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.3/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.4/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.4/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.5/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.5/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.6/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.7/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.7/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 1.8/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 1.9/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.0/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.0/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.1/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.1/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.2/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.2/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.3/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.4/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.4/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.5/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.5/10.8 MB 1.3 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.6/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 2.7/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 2.7/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 2.8/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 2.9/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.0/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.1/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.1/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 3.2/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.2/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.3/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.4/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 3.5/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.5/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.6/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.7/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.8/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 3.9/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 3.9/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 3.9/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.0/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.1/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.2/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.3/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 4.3/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.4/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.4/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.5/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.6/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.6/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.7/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.8/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 4.8/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 4.9/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 4.9/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.0/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 5.1/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.1/10.8 MB 1.4 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 5.2/10.8 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 5.3/10.8 MB 1.4 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 5.5/10.8 MB 1.4 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.7/10.8 MB 1.5 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 5.9/10.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 6.2/10.8 MB 1.6 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 6.4/10.8 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 6.6/10.8 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 6.8/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 6.9/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 7.1/10.8 MB 1.7 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 7.3/10.8 MB 1.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.5/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 7.8/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.9/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 8.1/10.8 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 8.3/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 8.5/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.7/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.8/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 8.9/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 9.0/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 9.1/10.8 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 9.3/10.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.5/10.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.7/10.8 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.8/10.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 9.9/10.8 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 10.1/10.8 MB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.3/10.8 MB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 10.4/10.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.8/10.8 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.8/10.8 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Requirement already satisfied: transformers in c:\\users\\usitha\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/ad/b1/6bb5006471264b5d75fcf0e3d7ed8d0bfc4ec335e08e05abf5900c42aa43/langchain-0.0.325-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.325-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (3.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/21/1f/1cff009cff64420572b9f75b70e4a054095719179a172297dfdd65843162/dataclasses_json-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.52 from https://files.pythonhosted.org/packages/72/82/b4b652719f72c0c2488de3fd0a9e14bb7ac952064d8171aa9ecc0dfdd8ab/langsmith-0.0.53-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.53-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from anyio<4.0->langchain) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
      "Using cached dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
      "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 langchain-0.0.325 langsmith-0.0.53 marshmallow-3.20.1 typing-inspect-0.9.0\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Installing collected packages: sentencepiece, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
      "Collecting InstructorEmbedding\n",
      "  Obtaining dependency information for InstructorEmbedding from https://files.pythonhosted.org/packages/6c/fc/64375441f43cc9ddc81f76a1a8f516e6d63f5b6ecb67fffdcddc0445f0d3/InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Using cached InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: InstructorEmbedding\n",
      "Successfully installed InstructorEmbedding-1.0.1\n",
      "Collecting textsum\n",
      "  Obtaining dependency information for textsum from https://files.pythonhosted.org/packages/4b/f7/33e0d34da2ab69f85839d59f62de088ebf820b32c4d22aea845c824f5f35/textsum-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading textsum-0.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting accelerate (from textsum)\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/d0/cf/364d550af711b5abe5129ac676896b223ba5a082d97fe400527a59c0c1f8/accelerate-0.24.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting clean-text (from textsum)\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting fire (from textsum)\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ---------------------------------------- 0.0/88.3 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/88.3 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 61.4/88.3 kB 812.7 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 88.3/88.3 kB 1.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting natsort (from textsum)\n",
      "  Obtaining dependency information for natsort from https://files.pythonhosted.org/packages/ef/82/7a9d0550484a62c6da82858ee9419f3dd1ccc9aa1c26a1e43da3ecd20b0d/natsort-8.4.0-py3-none-any.whl.metadata\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from textsum) (3.8.1)\n",
      "Requirement already satisfied: torch in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from textsum) (2.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from textsum) (4.65.0)\n",
      "Requirement already satisfied: transformers>=4.26.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from textsum) (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from transformers>=4.26.0->textsum) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from tqdm->textsum) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from accelerate->textsum) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch->textsum) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch->textsum) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch->textsum) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch->textsum) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from torch->textsum) (2023.4.0)\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text->textsum)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "     ---------------------------------------- 0.0/175.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 41.0/175.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 175.4/175.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0 (from clean-text->textsum)\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 0.0/53.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.1/53.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from fire->textsum) (1.16.0)\n",
      "Collecting termcolor (from fire->textsum)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from nltk->textsum) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from nltk->textsum) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text->textsum) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from jinja2->torch->textsum) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.26.0->textsum) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\usitha\\anaconda3\\lib\\site-packages (from sympy->torch->textsum) (1.3.0)\n",
      "Downloading textsum-0.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
      "   ---------------------------------------- 0.0/261.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 235.5/261.0 kB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 261.0/261.0 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: fire, emoji\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116947 sha256=a15fd7d94e95f27fed559f5fd675bb6741ab63b8dd5f0293545eec7ef2b7e10a\n",
      "  Stored in directory: c:\\users\\usitha\\appdata\\local\\pip\\cache\\wheels\\a7\\ee\\a5\\19e91481be8bea594935d137578bfe77d6bf905e4595336f6b\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171059 sha256=375821a7540114d20c27ca2cac64d4e31d670f6ff0abf9a3e7db8ea2ceb02f4f\n",
      "  Stored in directory: c:\\users\\usitha\\appdata\\local\\pip\\cache\\wheels\\bd\\22\\e5\\b69726d5e1a19795ecd3b3e7464b16c0f1d019aa94ff1c8578\n",
      "Successfully built fire emoji\n",
      "Installing collected packages: emoji, termcolor, natsort, ftfy, fire, clean-text, accelerate, textsum\n",
      "Successfully installed accelerate-0.24.0 clean-text-0.6.0 emoji-1.7.0 fire-0.5.0 ftfy-6.1.1 natsort-8.4.0 termcolor-2.3.0 textsum-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install replicate\n",
    "!pip install faiss-cpu\n",
    "# !pip install chromadb --progress-bar off\n",
    "!pip install transformers --progress-bar off\n",
    "!pip install langchain --progress-bar off\n",
    "!pip install sentence_transformers --progress-bar off\n",
    "!pip install InstructorEmbedding --progress-bar off\n",
    "!pip install textsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597f205bff474ea38cbb3f91f5742895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2dece2feda347adb5f3574a3f487363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a57eaa612a418eaad66d8726bc8d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe39b3d3e1954011ab46a9f9fcc68414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53f1d81f24d4320a6c11f52b3c16fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeed7f470b5b4673bf89d475fd8ba648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b521769cf249e3bc8e6e67c98cc7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2023 12:17:51 INFO Loaded model pszemraj/led-large-book-summary to cuda\n",
      "10/30/2023 12:17:51 INFO Load pretrained SentenceTransformer: hkunlp/instructor-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2023 12:17:56 WARNING Init param `input` is deprecated, please use `model_kwargs` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\ENGINEERING\\MORA\\ACA\\SEM5\\CS3501_Data_Science_and_Engineering_Project\\Project\\code\\Guten-bot\\flask_app.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=280'>281</a>\u001b[0m texts \u001b[39m=\u001b[39m loadForEmbeddings(book)\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m \u001b[39m## create embeddings\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m vectordb \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m     documents \u001b[39m=\u001b[39m texts,\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m     embedding \u001b[39m=\u001b[39m instructor_embeddings\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=286'>287</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m \u001b[39m# Variable to check whether the book name entered\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/f%3A/ENGINEERING/MORA/ACA/SEM5/CS3501_Data_Science_and_Engineering_Project/Project/code/Guten-bot/flask_app.ipynb#W3sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m no_book \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\langchain\\schema\\vectorstore.py:438\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    437\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 438\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\langchain\\vectorstores\\faiss.py:632\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    607\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    613\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m    614\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[0;32m    634\u001b[0m         texts,\n\u001b[0;32m    635\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    640\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\langchain\\embeddings\\huggingface.py:171\u001b[0m, in \u001b[0;36mHuggingFaceInstructEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute doc embeddings using a HuggingFace instruct model.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \n\u001b[0;32m    164\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m instruction_pairs \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_instruction, text] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[1;32m--> 171\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mencode(instruction_pairs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_kwargs)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:539\u001b[0m, in \u001b[0;36mINSTRUCTOR.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    536\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    538\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 539\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(features)\n\u001b[0;32m    541\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    542\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:269\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontext_masks\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[0;32m    268\u001b[0m     context_masks \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mcontext_masks\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 269\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrans_features, return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    270\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m    271\u001b[0m attention_mask \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1964\u001b[0m, in \u001b[0;36mT5EncoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1960\u001b[0m \u001b[39m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[0;32m   1961\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1962\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1964\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1965\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1966\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1967\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1968\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1969\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1970\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1971\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1972\u001b[0m )\n\u001b[0;32m   1974\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1046\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1042\u001b[0m     past_key_values \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock)\n\u001b[0;32m   1044\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[0;32m   1048\u001b[0m \u001b[39m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39mand\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Usitha\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:927\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    919\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape for input_ids (shape \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m) or attention_mask (shape \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m     )\n\u001b[0;32m    922\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[39m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[39m# positions we want to attend and the dtype's smallest value for masked positions.\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[1;32m--> 927\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n\u001b[0;32m    928\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mfinfo(dtype)\u001b[39m.\u001b[39mmin\n\u001b[0;32m    929\u001b[0m \u001b[39mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import langchain\n",
    "\n",
    "# loaders\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import Replicate\n",
    "from textsum.summarize import Summarizer\n",
    "\n",
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# from transformers import AutoTokenizer, TextStreamer, pipeline\n",
    "\n",
    "# data collectors\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import difflib\n",
    "\n",
    "\n",
    "# Models\n",
    "\n",
    "## Summarizing Model\n",
    "model_name = \"pszemraj/led-large-book-summary\"\n",
    "summarizer = Summarizer(\n",
    "    model_name_or_path=model_name,\n",
    "    token_batch_length=10000,\n",
    ")\n",
    "# configurations for summarizer\n",
    "min_word_count = 200\n",
    "max_word_count = 300\n",
    "\n",
    "tokens_per_word = 1.3\n",
    "\n",
    "min_token_count = min_word_count * tokens_per_word\n",
    "max_token_count = max_word_count * tokens_per_word\n",
    "\n",
    "# Set the length constraints in the inference params\n",
    "inference_params = summarizer.inference_params\n",
    "inference_params['max_length'] = int(max_token_count)\n",
    "inference_params['min_length'] = int(min_token_count)\n",
    "summarizer.set_inference_params(inference_params)\n",
    "\n",
    "## Embeddings model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = \"hkunlp/instructor-base\",\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "## Llama2-13 by Replicate\n",
    "REPLICATE_API_TOKEN = \"r8_4o6DI4Kl9VfQdrVv6OlaqvAyMhFdamr2jUDVe\"\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "\n",
    "llm = Replicate(\n",
    "    model = \"replicate/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf\",\n",
    "    input = {\"temperature\": 0.75, \"max_length\": 1024, \"top_p\": 0.95, \"repetition_penalty\": 1.15},\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Custom Prompt\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Functions for Book Retrieval\n",
    "\n",
    "## Function to search for a book by name and return the best match URL\n",
    "def search_book_by_name(book_name):\n",
    "    base_url = \"https://www.gutenberg.org/\"\n",
    "    search_url = base_url + \"ebooks/search/?query=\" + book_name.replace(\" \", \"+\") + \"&submit_search=Go%21\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the best match link based on similarity ratio\n",
    "    best_match_ratio = 0\n",
    "    best_match_url = \"\"\n",
    "\n",
    "    for link in soup.find_all(\"li\", class_=\"booklink\"):\n",
    "        link_title = link.find(\"span\", class_=\"title\").get_text()\n",
    "        similarity_ratio = difflib.SequenceMatcher(None, book_name.lower(), link_title.lower()).ratio()\n",
    "        if similarity_ratio > best_match_ratio:\n",
    "            best_match_ratio = similarity_ratio\n",
    "            best_match_url = base_url + link.find(\"a\").get(\"href\")\n",
    "\n",
    "    return best_match_url\n",
    "\n",
    "## Function to get the \"Plain Text UTF-8\" download link from the book page\n",
    "def get_plain_text_link(book_url):\n",
    "    response = requests.get(book_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    plain_text_link = \"\"\n",
    "\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        format_cell = row.find(\"td\", class_=\"unpadded icon_save\")\n",
    "        if format_cell and \"Plain Text UTF-8\" in format_cell.get_text():\n",
    "            plain_text_link = format_cell.find(\"a\").get(\"href\")\n",
    "            break\n",
    "\n",
    "    return plain_text_link\n",
    "\n",
    "\n",
    "## Function to get the content of the \"Plain Text UTF-8\" link\n",
    "def get_plain_text_content(plain_text_link):\n",
    "    response = requests.get(plain_text_link)\n",
    "    content = response.text\n",
    "    return content\n",
    "\n",
    "\n",
    "## Main function\n",
    "def load_book(book_name):\n",
    "    best_match_url = search_book_by_name(book_name)\n",
    "\n",
    "    if best_match_url:\n",
    "        plain_text_link = get_plain_text_link(best_match_url)\n",
    "        if plain_text_link:\n",
    "            full_plain_text_link = \"https://www.gutenberg.org\" + plain_text_link\n",
    "            plain_text_content = get_plain_text_content(full_plain_text_link)\n",
    "#             print(\"Plain Text UTF-8 content:\", plain_text_content)\n",
    "\n",
    "            book_text = plain_text_content\n",
    "\n",
    "            # Remove the BOM character if it exists\n",
    "            book_text = book_text.lstrip('\\ufeff')\n",
    "\n",
    "            #####\n",
    "             # Define the possible variations of the start marker\n",
    "            possible_start_markers = [\n",
    "                r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK (.+?) \\*\\*\\*\",\n",
    "                r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK (.+?) \\*\\*\\*\"\n",
    "            ]\n",
    "\n",
    "            # Fetch the plain_text_content of the book (assuming you have it)\n",
    "            plain_text_content = book_text  # Fetch the content here\n",
    "\n",
    "            start_index = None\n",
    "            for start_marker_pattern in possible_start_markers:\n",
    "                match = re.search(start_marker_pattern, book_text)\n",
    "                if match:\n",
    "                    start_index = match.start()\n",
    "                    book_name = match.group(1)\n",
    "                    break\n",
    "\n",
    "            if start_index is not None:\n",
    "                end_marker = f\"*** END OF THE PROJECT GUTENBERG EBOOK {book_name} ***\"\n",
    "\n",
    "                end_index = plain_text_content.find(end_marker, start_index)\n",
    "\n",
    "                if end_index != -1:\n",
    "                    book_text = plain_text_content[start_index + len(match.group(0)):end_index]\n",
    "\n",
    "\n",
    "            #####\n",
    "\n",
    "            # Choose an appropriate encoding, such as 'utf-8'\n",
    "            with open(\"book.txt\", \"w\", encoding=\"utf-8\") as book:\n",
    "                book.write(book_text)\n",
    "\n",
    "            return book_text\n",
    "        else:\n",
    "            print(\"No Plain Text UTF-8 link found.\")\n",
    "            return \"web site error\"\n",
    "    else:\n",
    "        print(\"No matching book found.\")\n",
    "        return \"web site error\"\n",
    "\n",
    "\n",
    "# Function to get Summary\n",
    "def generate_summary(book_text):\n",
    "  global summarizer\n",
    "  out_str = summarizer.summarize_string(book_text)\n",
    "  # input_ids = tokenizer.encode(out_str, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "  # summary_ids = model.generate(input_ids, max_length=300, min_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "  # summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  # return summary\n",
    "  return out_str\n",
    "\n",
    "\n",
    "# Functions for Q/A chatbot\n",
    "\n",
    "## Splitting book.txt to create embeddings\n",
    "def loadForEmbeddings(txt_file):\n",
    "    # load document\n",
    "    loader = TextLoader(txt_file, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 800,\n",
    "        chunk_overlap = 0\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    return texts\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=200): # 110\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "## Format llm response\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "\n",
    "    sources_used = llm_response['source_documents'][0].metadata['source']\n",
    "\n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans\n",
    "\n",
    "## Main function in Q/A\n",
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "        retriever = retriever,\n",
    "        chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "        return_source_documents = True,\n",
    "        verbose = False\n",
    "    )\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str\n",
    "\n",
    "# Example for creating Embeddings\n",
    "book_name = \"The prince\"\n",
    "book_text = load_book(book_name)\n",
    "book = \"book.txt\"\n",
    "texts = loadForEmbeddings(book)\n",
    "\n",
    "## create embeddings\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = texts,\n",
    "    embedding = instructor_embeddings\n",
    ")\n",
    "\n",
    "# Variable to check whether the book name entered\n",
    "no_book = False\n",
    "\n",
    "# Gets book name\n",
    "# then creates embeddings\n",
    "# and after that generates and returns summary\n",
    "def submit_book(book_name):\n",
    "    global vectordb, retriever, instructor_embeddings, no_book\n",
    "    if not book_name:\n",
    "      no_book = True\n",
    "      return \"Please enter the name of the book.\"\n",
    "\n",
    "    book_text = load_book(book_name)\n",
    "    book = \"book.txt\"\n",
    "    texts = loadForEmbeddings(book)\n",
    "\n",
    "    # create embeddings\n",
    "    vectordb = FAISS.from_documents(\n",
    "        documents = texts,\n",
    "        embedding = instructor_embeddings\n",
    "    )\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})\n",
    "\n",
    "    summary = generate_summary(book_text)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Gets the prompt and returns Llm response\n",
    "def get_response(prompt):\n",
    "    if (no_book and not prompt):\n",
    "        return \"Please enter the name of the book and the prompt.\"\n",
    "    if no_book:\n",
    "        return \"Please enter the name of the book.\"\n",
    "    if not prompt:\n",
    "        return \"Please enter the prompt.\"\n",
    "\n",
    "    return llm_ans(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response_ = submit_book(\"The Valley of Fear\")\n",
    "print(response_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "answer = get_response(\"What are the characters in the book?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
