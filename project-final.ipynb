{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! nvidia-smi -L","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T20:43:41.229701Z","iopub.execute_input":"2023-10-28T20:43:41.230384Z","iopub.status.idle":"2023-10-28T20:43:42.238092Z","shell.execute_reply.started":"2023-10-28T20:43:41.230337Z","shell.execute_reply":"2023-10-28T20:43:42.237170Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-253ad06d-fd98-9572-e9bb-c87873db54d4)\nGPU 1: Tesla T4 (UUID: GPU-ffd7f6b9-80cf-5b0d-8951-ae3bd6ff671f)\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# both\n!pip install -qqq transformers==4.31.0 --progress-bar off\n!pip install -qqq langchain==0.0.266 --progress-bar off\n# !pip install -qqq xformers==0.0.20 --progress-bar off\n!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n!pip install -qqq InstructorEmbedding==1.0.1 --progress-bar off\n\n# th\n!pip install -Uqqq pip --progress-bar off\n!pip install -qqq torch==2.0.1 --progress-bar off\n# !pip install -qqq chromadb==0.4.5 --progress-bar off\n# !pip install -qqq pypdf==3.15.0 --progress-bar off\n# !pip install -qqq pdf2image==1.16.3 --progress-bar off\n\n\n# my\n! pip install -qq -U faiss-cpu\n# ! pip install -qq -U accelerate bitsandbytes einops tiktoken","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:43:42.239976Z","iopub.execute_input":"2023-10-28T20:43:42.240324Z","iopub.status.idle":"2023-10-28T20:48:12.523779Z","shell.execute_reply.started":"2023-10-28T20:43:42.240291Z","shell.execute_reply":"2023-10-28T20:48:12.522675Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCPU times: user 3.58 s, sys: 831 ms, total: 4.42 s\nWall time: 4min 30s\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:12.525164Z","iopub.execute_input":"2023-10-28T20:48:12.525480Z","iopub.status.idle":"2023-10-28T20:48:13.907540Z","shell.execute_reply.started":"2023-10-28T20:48:12.525451Z","shell.execute_reply":"2023-10-28T20:48:13.906502Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:13.909939Z","iopub.execute_input":"2023-10-28T20:48:13.910248Z","iopub.status.idle":"2023-10-28T20:48:27.083349Z","shell.execute_reply.started":"2023-10-28T20:48:13.910217Z","shell.execute_reply":"2023-10-28T20:48:27.082238Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# !sudo apt-get install poppler-utils","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:27.084760Z","iopub.execute_input":"2023-10-28T20:48:27.085027Z","iopub.status.idle":"2023-10-28T20:48:27.089668Z","shell.execute_reply.started":"2023-10-28T20:48:27.085000Z","shell.execute_reply":"2023-10-28T20:48:27.088611Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport glob\nimport textwrap\nimport time\n\nimport langchain\n\n# loaders\n# from langchain.document_loaders import PyPDFLoader\n# from langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import TextLoader\n\n\n# splits\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# prompts\nfrom langchain import PromptTemplate, LLMChain\n\n# vector stores\nfrom langchain.vectorstores import FAISS\n\n# models\nfrom langchain.llms import HuggingFacePipeline\nfrom InstructorEmbedding import INSTRUCTOR\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\n\n# retrievers\nfrom langchain.chains import RetrievalQA\n\nimport torch\nimport transformers\n# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom transformers import AutoTokenizer, TextStreamer, pipeline\n\n\nprint(langchain.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:27.090805Z","iopub.execute_input":"2023-10-28T20:48:27.091058Z","iopub.status.idle":"2023-10-28T20:48:50.689026Z","shell.execute_reply.started":"2023-10-28T20:48:27.091035Z","shell.execute_reply":"2023-10-28T20:48:50.688044Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0.0.266\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\nfrom auto_gptq import AutoGPTQForCausalLM\n# import torch\n# from langchain import HuggingFacePipeline, PromptTemplate\n# from langchain.chains import RetrievalQA\n# from langchain.document_loaders import PyPDFDirectoryLoader\n# from langchain.embeddings import HuggingFaceInstructEmbeddings\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n# from pdf2image import convert_from_path\n# from transformers import AutoTokenizer, TextStreamer, pipeline\n\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:50.690191Z","iopub.execute_input":"2023-10-28T20:48:50.690505Z","iopub.status.idle":"2023-10-28T20:48:51.849012Z","shell.execute_reply.started":"2023-10-28T20:48:50.690481Z","shell.execute_reply":"2023-10-28T20:48:51.848060Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\nmodel_basename = \"model\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\n    model_name_or_path,\n    revision=\"gptq-4bit-128g-actorder_True\",\n    model_basename=model_basename,\n    use_safetensors=True,\n    trust_remote_code=True,\n    inject_fused_attention=False,\n    device=DEVICE,\n    quantize_config=None,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:48:51.850912Z","iopub.execute_input":"2023-10-28T20:48:51.851305Z","iopub.status.idle":"2023-10-28T20:49:43.853699Z","shell.execute_reply.started":"2023-10-28T20:48:51.851269Z","shell.execute_reply":"2023-10-28T20:49:43.852637Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125d059c9bcc46faaffd3b576f12cffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8b983d4c424a219c0ba0235a7a0d8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d99b7b92e24b5290fcbf8fcb0c6d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a6ac8d5da104d75a3a0150bdf824db9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40452588d985489c8fc59919e1c9a2a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)der_True/config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e6c28189a24534875953c8d5350682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)quantize_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93efbcf637a5454f83f05439df4d08fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290d7d12d1da461d9055a47f1f461ced"}},"metadata":{}}]},{"cell_type":"code","source":"# max_len = 8192","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:43.855038Z","iopub.execute_input":"2023-10-28T20:49:43.855360Z","iopub.status.idle":"2023-10-28T20:49:43.860450Z","shell.execute_reply.started":"2023-10-28T20:49:43.855332Z","shell.execute_reply":"2023-10-28T20:49:43.859381Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     # LLMs\n#     model_name = 'llama2-13b' # wizardlm, bloom, falcon, llama2-7b, llama2-13b\n#     temperature = 0,\n#     top_p = 0.95,\n#     repetition_penalty = 1.15    \n\n#     # splitting\n#     split_chunk_size = 800\n#     split_overlap = 0\n    \n#     # embeddings\n#     embeddings_model_repo = 'hkunlp/instructor-base'    \n\n#     # similar passages\n#     k = 3","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:43.865532Z","iopub.execute_input":"2023-10-28T20:49:43.865857Z","iopub.status.idle":"2023-10-28T20:49:46.065434Z","shell.execute_reply.started":"2023-10-28T20:49:43.865831Z","shell.execute_reply":"2023-10-28T20:49:46.064318Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:46.066851Z","iopub.execute_input":"2023-10-28T20:49:46.067498Z","iopub.status.idle":"2023-10-28T20:49:46.137585Z","shell.execute_reply.started":"2023-10-28T20:49:46.067441Z","shell.execute_reply":"2023-10-28T20:49:46.136205Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_repo = 'daryl149/llama-2-13b-chat-hf'\n        \n# tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_repo,\n#     load_in_4bit=True,\n#     device_map='auto',\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n#     trust_remote_code=True\n# )\n        \n# max_len = 8192","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:46.139176Z","iopub.execute_input":"2023-10-28T20:49:46.139954Z","iopub.status.idle":"2023-10-28T20:49:46.209185Z","shell.execute_reply.started":"2023-10-28T20:49:46.139915Z","shell.execute_reply":"2023-10-28T20:49:46.208045Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:46.210677Z","iopub.execute_input":"2023-10-28T20:49:46.211063Z","iopub.status.idle":"2023-10-28T20:49:46.299193Z","shell.execute_reply.started":"2023-10-28T20:49:46.211026Z","shell.execute_reply":"2023-10-28T20:49:46.298082Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=1024,\n    temperature=0,\n    top_p=0.95,\n    repetition_penalty=1.15,\n    streamer=streamer,\n)\n\nllm = HuggingFacePipeline(pipeline = pipe, model_kwargs={\"temperature\": 0})","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:46.300531Z","iopub.execute_input":"2023-10-28T20:49:46.300847Z","iopub.status.idle":"2023-10-28T20:49:46.634500Z","shell.execute_reply.started":"2023-10-28T20:49:46.300821Z","shell.execute_reply":"2023-10-28T20:49:46.633562Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\nThe model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"### testing model, not using the book yet\n### answer is not necessarily related to the book\nquery = \"Give me 5 examples of cool potions and explain what they do\"\nllm(query)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:49:46.635811Z","iopub.execute_input":"2023-10-28T20:49:46.636353Z","iopub.status.idle":"2023-10-28T20:50:07.905279Z","shell.execute_reply.started":"2023-10-28T20:49:46.636320Z","shell.execute_reply":"2023-10-28T20:50:07.904354Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":".\n\nSure thing! Here are five examples of cool potions that you might find in a fantasy world, along with their effects:\n\n1. Potion of Healing: This potion restores health to the drinker, healing wounds and injuries. It might also have a soothing effect on the mind, reducing stress and fatigue.\n2. Potion of Strength: This potion grants the drinker temporary superhuman strength, allowing them to lift heavy objects or perform feats of physical prowess. It might also increase endurance, allowing the drinker to work or fight for longer periods without tiring.\n3. Potion of Speed: This potion increases the drinker's speed and agility, allowing them to move quickly and dodge attacks more easily. It might also grant the ability to run faster, jump higher, or move through difficult terrain with ease.\n4. Potion of Invisibility: This potion makes the drinker temporarily invisible, allowing them to sneak past enemies undetected or escape dangerous situations unnoticed. It might also grant the ability to become partially visible at will, allowing the drinker to reveal themselves when desired.\n5. Potion of Illusion: This potion creates powerful illusions in the minds of those who drink it, making them see or believe things that aren't really there. It might allow the drinker to create fake environments, summon fictional creatures, or make themselves appear differently to others. The effects of this potion could be used for entertainment, deception, or self-defense.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\".\\n\\nSure thing! Here are five examples of cool potions that you might find in a fantasy world, along with their effects:\\n\\n1. Potion of Healing: This potion restores health to the drinker, healing wounds and injuries. It might also have a soothing effect on the mind, reducing stress and fatigue.\\n2. Potion of Strength: This potion grants the drinker temporary superhuman strength, allowing them to lift heavy objects or perform feats of physical prowess. It might also increase endurance, allowing the drinker to work or fight for longer periods without tiring.\\n3. Potion of Speed: This potion increases the drinker's speed and agility, allowing them to move quickly and dodge attacks more easily. It might also grant the ability to run faster, jump higher, or move through difficult terrain with ease.\\n4. Potion of Invisibility: This potion makes the drinker temporarily invisible, allowing them to sneak past enemies undetected or escape dangerous situations unnoticed. It might also grant the ability to become partially visible at will, allowing the drinker to reveal themselves when desired.\\n5. Potion of Illusion: This potion creates powerful illusions in the minds of those who drink it, making them see or believe things that aren't really there. It might allow the drinker to create fake environments, summon fictional creatures, or make themselves appear differently to others. The effects of this potion could be used for entertainment, deception, or self-defense.\""},"metadata":{}}]},{"cell_type":"code","source":"prompt_template = \"\"\"\nDon't try to make up an answer, if you don't know just say that you don't know.\nAnswer in the same language the question was asked.\nUse only the following pieces of context to answer the question at the end.\n\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\n\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:07.906390Z","iopub.execute_input":"2023-10-28T20:50:07.906674Z","iopub.status.idle":"2023-10-28T20:50:07.911590Z","shell.execute_reply.started":"2023-10-28T20:50:07.906650Z","shell.execute_reply":"2023-10-28T20:50:07.910716Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def loadForEmbeddings(book_txt):\n    # load document\n    loader = TextLoader(book_txt, encoding=\"utf-8\")\n    documents = loader.load()\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size = 800,\n        chunk_overlap = 0\n    )\n\n    texts = text_splitter.split_documents(documents)\n    return texts\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:07.912968Z","iopub.execute_input":"2023-10-28T20:50:07.913314Z","iopub.status.idle":"2023-10-28T20:50:07.926748Z","shell.execute_reply.started":"2023-10-28T20:50:07.913277Z","shell.execute_reply":"2023-10-28T20:50:07.925833Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=200): # 110\n    # Split the input text into lines based on newline characters\n    lines = text.split('\\n')\n\n    # Wrap each line individually\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n\n    # Join the wrapped lines back together using newline characters\n    wrapped_text = '\\n'.join(wrapped_lines)\n\n    return wrapped_text\n\n\ndef process_llm_response(llm_response):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n    \n    sources_used = llm_response['source_documents'][0].metadata['source']\n    \n    ans = ans + '\\n\\nSources: \\n' + sources_used\n    return ans","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:07.928058Z","iopub.execute_input":"2023-10-28T20:50:07.928557Z","iopub.status.idle":"2023-10-28T20:50:07.938960Z","shell.execute_reply.started":"2023-10-28T20:50:07.928523Z","shell.execute_reply":"2023-10-28T20:50:07.938078Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def llm_ans(query):\n    start = time.time()\n    qa_chain = RetrievalQA.from_chain_type(\n        llm = llm,\n        chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n        retriever = retriever, \n        chain_type_kwargs = {\"prompt\": PROMPT},\n        return_source_documents = True,\n        verbose = False\n    )\n    llm_response = qa_chain(query)\n    ans = process_llm_response(llm_response)\n    end = time.time()\n\n    time_elapsed = int(round(end - start, 0))\n    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n    return ans + time_elapsed_str","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:07.940169Z","iopub.execute_input":"2023-10-28T20:50:07.940534Z","iopub.status.idle":"2023-10-28T20:50:07.951168Z","shell.execute_reply.started":"2023-10-28T20:50:07.940502Z","shell.execute_reply":"2023-10-28T20:50:07.950323Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport difflib\n\n# Function to search for a book by name and return the best match URL\ndef search_book_by_name(book_name):\n    base_url = \"https://www.gutenberg.org/\"\n    search_url = base_url + \"ebooks/search/?query=\" + book_name.replace(\" \", \"+\") + \"&submit_search=Go%21\"\n    \n    response = requests.get(search_url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # Find the best match link based on similarity ratio\n    best_match_ratio = 0\n    best_match_url = \"\"\n\n    for link in soup.find_all(\"li\", class_=\"booklink\"):\n        link_title = link.find(\"span\", class_=\"title\").get_text()\n        similarity_ratio = difflib.SequenceMatcher(None, book_name.lower(), link_title.lower()).ratio()\n        if similarity_ratio > best_match_ratio:\n            best_match_ratio = similarity_ratio\n            best_match_url = base_url + link.find(\"a\").get(\"href\")\n\n    return best_match_url\n\n# Function to get the \"Plain Text UTF-8\" download link from the book page\ndef get_plain_text_link(book_url):\n    response = requests.get(book_url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    plain_text_link = \"\"\n    \n    for row in soup.find_all(\"tr\"):\n        format_cell = row.find(\"td\", class_=\"unpadded icon_save\")\n        if format_cell and \"Plain Text UTF-8\" in format_cell.get_text():\n            plain_text_link = format_cell.find(\"a\").get(\"href\")\n            break\n    \n    return plain_text_link\n\n\n# Function to get the content of the \"Plain Text UTF-8\" link\ndef get_plain_text_content(plain_text_link):\n    response = requests.get(plain_text_link)\n    content = response.text\n    return content\n\n\n# Main function\ndef load_book(book_name):\n    best_match_url = search_book_by_name(book_name)\n\n    if best_match_url:\n        plain_text_link = get_plain_text_link(best_match_url)\n        if plain_text_link:\n            full_plain_text_link = \"https://www.gutenberg.org\" + plain_text_link\n            plain_text_content = get_plain_text_content(full_plain_text_link)\n#             print(\"Plain Text UTF-8 content:\", plain_text_content)\n            \n            book_text = plain_text_content\n            \n            file = book_name + \".txt\"\n\n            # Remove the BOM character if it exists\n            book_text = book_text.lstrip('\\ufeff')\n\n            # Choose an appropriate encoding, such as 'utf-8'\n            with open(file, \"w\", encoding=\"utf-8\") as file:\n                file.write(book_text)\n                \n            return book_text\n        else:\n            print(\"No Plain Text UTF-8 link found.\")\n            return \"web site error\"\n    else:\n        print(\"No matching book found.\")\n        return \"web site error\"\n\n# def clean_book_content(book_text):\n#     cleaned_book_text = book_text.replace(\"\\n\", \" \")\n#     cleaned_book_text = cleaned_book_text.replace(\"\\r\", \" \")\n#     cleaned_book_text = cleaned_book_text.replace(\"\\ufeff\", \"\")\n#     return cleaned_book_text","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:07.952482Z","iopub.execute_input":"2023-10-28T20:50:07.952842Z","iopub.status.idle":"2023-10-28T20:50:08.223393Z","shell.execute_reply.started":"2023-10-28T20:50:07.952810Z","shell.execute_reply":"2023-10-28T20:50:08.222518Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"book_name = \"The prince\"\nbook_text = load_book(book_name)\nfile = book_name + \".txt\"\ntexts = loadForEmbeddings(file)\n\n### download embeddings model\ninstructor_embeddings = HuggingFaceInstructEmbeddings(\n    model_name = 'hkunlp/instructor-base',\n    model_kwargs = {\"device\": \"cuda\"}\n)\n\n### create embeddings and DB\nvectordb = FAISS.from_documents(\n    documents = texts, \n    embedding = instructor_embeddings\n)\n\n### persist vector database\nvectordb.save_local(\"faiss_index_hp\")\n    \nretriever = vectordb.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:08.224453Z","iopub.execute_input":"2023-10-28T20:50:08.225099Z","iopub.status.idle":"2023-10-28T20:50:20.062938Z","shell.execute_reply.started":"2023-10-28T20:50:08.225072Z","shell.execute_reply":"2023-10-28T20:50:20.062188Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)62736/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b07c1f821f0e4f1ea58c4c36d98af800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab26fb8b9a841ef834066c3cb21f730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/2_Dense/config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9cfe8eadf164dd399009ddd6915ea07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d894e9bab35c4f22810f3f6a8f6b16ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)15e6562736/README.md:   0%|          | 0.00/66.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c272cc97577849008869ed1f405ef7e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)e6562736/config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc236057c6845b9bb670e499cb65786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f3a7ee2b5e4654b1bed540df9767b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1c58ac89734a77aa0f31b38935c58e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c018f212da465baf1f02ec4018f417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f14d1f949004ab5915199bbf4a10dc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca00b654308c46d29d81cbb816590e3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)62736/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf107dd4809d471cb75a8fc7e971ca6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1570f62b3cd4e61be6f8475fba0f66e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)6562736/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd2b0da06df48f9bcf96a6b442d2dc5"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"llm_ans(\"What are the characters\")","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:20.064031Z","iopub.execute_input":"2023-10-28T20:50:20.064419Z","iopub.status.idle":"2023-10-28T20:50:21.600536Z","shell.execute_reply.started":"2023-10-28T20:50:20.064391Z","shell.execute_reply":"2023-10-28T20:50:21.599629Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"The characters mentioned are princes, the people, and mercenary captains.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"' The characters mentioned are princes, the people, and mercenary captains.\\n\\nSources: \\nThe prince.txt\\n\\nTime elapsed: 2 s'"},"metadata":{}}]},{"cell_type":"code","source":"# pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:21.601802Z","iopub.execute_input":"2023-10-28T20:50:21.602501Z","iopub.status.idle":"2023-10-28T20:50:21.606389Z","shell.execute_reply.started":"2023-10-28T20:50:21.602464Z","shell.execute_reply":"2023-10-28T20:50:21.605525Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# import gradio as gr\n\n# title = \"GutenbergChat Hub\"\n# vectordb = None\n# retriever = None\n\n# # Submit book\n# def submit_book(book_name):\n#     global vectordb, retriever\n#     if not book_name:\n#         return \"Please enter the name of the book.\"\n    \n#     book_text = load_book(book_name)\n#     file = book_name + \".txt\"\n#     texts = loadForEmbeddings(file)\n\n#     ### download embeddings model\n#     instructor_embeddings = HuggingFaceInstructEmbeddings(\n#         model_name = 'hkunlp/instructor-base',\n#         model_kwargs = {\"device\": \"cuda\"}\n#     )\n\n#     ### create embeddings and DB\n#     vectordb = FAISS.from_documents(\n#         documents = texts, \n#         embedding = instructor_embeddings\n#     )\n\n#     ### persist vector database\n#     vectordb.save_local(\"faiss_index_hp\")\n    \n#     retriever = vectordb.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})\n\n    \n#     return \"done\"\n\n\n# # Q/A\n# def get_response(prompt):\n# #     if (not book_name and not prompt):\n# #         return \"Please enter the name of the book and the prompt.\"\n# #     if not book_name:\n# #         return \"Please enter the name of the book.\"\n# #     if not prompt:\n# #         return \"Please enter the prompt.\"\n#     query = prompt\n# #     llm_response = qa_chain(query)\n    \n#     return llm_ans(query)\n\n# # Interface 1\n# submitBook = gr.Interface(fn=submit_book, inputs=\"text\", outputs=\"text\", title=\"Submit your book here first\")\n# # Interface 2\n# chatBot = gr.Interface(\n#         fn=get_response,\n#         inputs=\"text\",\n#         outputs=\"text\",\n#         title=title + \" - Q/A-Bot\",\n#         description=\"Enter the name of the book in previous tab, then ask questions here\",\n#         examples=[\"What are the characters in the book?\"]\n#     )\n\n# demo = gr.TabbedInterface([submitBook, chatBot], [\"SubmitBook\", \"Q/A-Bot\"])\n# demo.launch()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T20:50:21.607778Z","iopub.execute_input":"2023-10-28T20:50:21.608387Z","iopub.status.idle":"2023-10-28T20:50:21.623060Z","shell.execute_reply.started":"2023-10-28T20:50:21.608353Z","shell.execute_reply":"2023-10-28T20:50:21.622181Z"},"trusted":true},"execution_count":24,"outputs":[]}]}